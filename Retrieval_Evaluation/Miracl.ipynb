{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/miracl--miracl/f598b4ee332f2b16e82c6c83ab1ba82e1a7777ef82e7ce3c1416f6b20a142313 (last modified on Fri Nov 17 04:13:02 2023) since it couldn't be found locally at miracl/miracl., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset miracl (/root/.cache/huggingface/datasets/miracl___miracl/th/1.0.0/f598b4ee332f2b16e82c6c83ab1ba82e1a7777ef82e7ce3c1416f6b20a142313)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391654149abd4d9b99bb43372a5f8568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang='th'  # or any of the 16 languages\n",
    "miracl = datasets.load_dataset('miracl/miracl', lang, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_query = []\n",
    "all_answers = []\n",
    "all_text = []\n",
    "for data in miracl['dev']: \n",
    "    query_id = data['query_id']\n",
    "    query = data['query']\n",
    "    positive_passages = data['positive_passages']\n",
    "    negative_passages = data['negative_passages']\n",
    "    \n",
    "    all_query.append(query)\n",
    "    all_answers.append([x['text'] for x in positive_passages])\n",
    "   \n",
    "    all_text += [x['text'] for x in positive_passages]\n",
    "    all_text += [x['text'] for x in negative_passages]\n",
    "all_text = list(set(all_text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(docs, doc_embeddings, answers, question_embeddings,mrr_rank=10):\n",
    "    # docs : all docs [d1,d2,d3]\n",
    "    # doc_embeddings : embeddings from all docs => [e1,e2,e3,...]\n",
    "    # answers : a set of answers => [[a1,a2],[a1,a2,a3]]\n",
    "    # question_embeddings: a embedding from all questions => [e1,e2,e3,...]\n",
    "    top_1 = 0; top_5 = 0; top_10 = 0;\n",
    "    mrr_score = 0\n",
    "    sim_score = np.inner(question_embeddings,doc_embeddings)\n",
    "    status_bar = enumerate(sim_score)\n",
    "    for idx,sim in status_bar:\n",
    "        index = np.argsort(sim)[::-1]\n",
    "        doc_sorted = [docs[i] for i in index]\n",
    "        answer_idx = [doc_sorted.index(a) for a in answers[idx]] # cal index for each answer\n",
    "        final_idx_search = min(answer_idx) # since we have multiple answers, we find the min index! \n",
    "        if final_idx_search == 0:\n",
    "            top_1+=1\n",
    "            top_5+=1\n",
    "            top_10+=1\n",
    "        elif final_idx_search < 5:\n",
    "            top_5+=1\n",
    "            top_10+=1\n",
    "        elif final_idx_search < 10:\n",
    "            top_10+=1  \n",
    "        if final_idx_search < mrr_rank:\n",
    "            mrr_score += (1/(final_idx_search+1))\n",
    "    mrr_score/=len(question_embeddings)\n",
    "    return top_1,top_5,top_10,mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "Traninng Score P@1: 0.3820\n",
      "Traninng Score P@5: 0.6576\n",
      "Traninng Score P@10: 0.7299\n",
      "Mrr score:0.4965\n"
     ]
    }
   ],
   "source": [
    "doc_context_encoded = model.encode(all_text,convert_to_numpy=True,normalize_embeddings=True)\n",
    "questions = model.encode(all_query,convert_to_numpy=True,normalize_embeddings=True)\n",
    "\n",
    "top_1,top_5,top_10,mrr = evaluate(all_text,doc_context_encoded,all_answers,questions)\n",
    "print(f'{model_name}')\n",
    "precision = top_1 / len(questions)\n",
    "print(f\"Traninng Score P@1: {precision:.4f}\")\n",
    "precision = top_5 / len(questions)\n",
    "print(f\"Traninng Score P@5: {precision:.4f}\")\n",
    "precision = top_10 / len(questions)\n",
    "print(f\"Traninng Score P@10: {precision:.4f}\")\n",
    "print(f\"Mrr score:{mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client('YOUR COHERE API KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traninng Score P@1: 0.6698\n",
      "Traninng Score P@5: 0.9141\n",
      "Traninng Score P@10: 0.9495\n",
      "Mrr score:0.7758\n"
     ]
    }
   ],
   "source": [
    "bs = 96\n",
    "query = []\n",
    "doc = []\n",
    "for i in range(len(all_text)//bs+1):\n",
    "    doc.append(co.embed(\n",
    "      texts=all_text[(i*bs):((i+1)*bs)],\n",
    "      model='embed-multilingual-v2.0',\n",
    "    ).embeddings)\n",
    "for i in range(len(all_query)//bs+1):\n",
    "    query.append(co.embed(\n",
    "      texts=all_query[(i*bs):((i+1)*bs)],\n",
    "      model='embed-multilingual-v2.0',\n",
    "    ).embeddings)\n",
    "\n",
    "questions = np.concatenate(query,0)\n",
    "doc_context_encoded = np.concatenate(doc,0)\n",
    "\n",
    "top_1,top_5,top_10,mrr = evaluate(all_text,doc_context_encoded,all_answers,questions)\n",
    "precision = top_1 / len(questions)\n",
    "print(f\"Traninng Score P@1: {precision:.4f}\")\n",
    "precision = top_5 / len(questions)\n",
    "print(f\"Traninng Score P@5: {precision:.4f}\")\n",
    "precision = top_10 / len(questions)\n",
    "print(f\"Traninng Score P@10: {precision:.4f}\")\n",
    "print(f\"Mrr score:{mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
