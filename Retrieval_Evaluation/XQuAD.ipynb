{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]=\"/workspace/cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"]=\"/workspace/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.6k/15.6k [00:00<00:00, 15.9MB/s]\n",
      "Downloading data files:   0%|                                                                                                                                                                                                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                                                                                                              | 0.00/337k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 337k/337k [00:02<00:00, 132kB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.56s/it]\n",
      "Extracting data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 917.39it/s]\n",
      "Generating validation split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1190/1190 [00:00<00:00, 62902.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"xquad\",\"xquad.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc = set(dataset['validation']['context'])\n",
    "all_doc = {c:i for i,c in enumerate(all_doc)}\n",
    "\n",
    "question_contextid_context = []\n",
    "for item in dataset['validation']:\n",
    "    question = item['question']\n",
    "    doc = item['context']\n",
    "    question_contextid_context.append([all_doc[doc],question])\n",
    "    \n",
    "df_question = pd.DataFrame(question_contextid_context, columns =['doc_id','question'])\n",
    "df_document = pd.DataFrame(zip(list(all_doc.values()),list(all_doc.keys())), columns =['doc_id','document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(question_id,question_all,context_id,context_all,mrr_rank=10,status=True):\n",
    "    top_1 = 0; top_5 = 0; top_10 = 0;\n",
    "    mrr_score = 0\n",
    "    context_id = np.array(context_id)\n",
    "    sim_score = np.inner(question_all,context_all)\n",
    "    status_bar = enumerate(sim_score)\n",
    "    for idx,sim in status_bar:\n",
    "        index = np.argsort(sim)[::-1]\n",
    "        index_edit = [context_id[x] for x in index]\n",
    "        idx_search = list(index_edit).index(question_id[idx])\n",
    "        if idx_search == 0:\n",
    "            top_1+=1\n",
    "            top_5+=1\n",
    "            top_10+=1\n",
    "        elif idx_search < 5:\n",
    "            top_5+=1\n",
    "            top_10+=1\n",
    "        elif idx_search < 10:\n",
    "            top_10+=1  \n",
    "        if idx_search < mrr_rank:\n",
    "            mrr_score += (1/(idx_search+1))\n",
    "    mrr_score/=len(question_all)\n",
    "    return top_1,top_5,top_10,mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' # mrp/simcse-model-distil-m-bert\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "Traninng Score P@1: 0.7126\n",
      "Traninng Score P@5: 0.9092\n",
      "Traninng Score P@10: 0.9496\n",
      "Mrr score:0.7963\n"
     ]
    }
   ],
   "source": [
    "doc_context_id = df_document['doc_id'].to_list()    \n",
    "doc_context_encoded = model.encode(df_document['document'].to_list(),convert_to_numpy=True,normalize_embeddings=True)\n",
    "\n",
    "question_id = df_question['doc_id'].to_list()\n",
    "questions = model.encode(df_question['question'].to_list(),convert_to_numpy=True,normalize_embeddings=True)\n",
    "\n",
    "top_1,top_5,top_10,mrr = evaluate(question_id,questions,doc_context_id,doc_context_encoded)\n",
    "\n",
    "print(f'{model_name}')\n",
    "precision = top_1 / len(questions)\n",
    "print(f\"Traninng Score P@1: {precision:.4f}\")\n",
    "precision = top_5 / len(questions)\n",
    "print(f\"Traninng Score P@5: {precision:.4f}\")\n",
    "precision = top_10 / len(questions)\n",
    "print(f\"Traninng Score P@10: {precision:.4f}\")\n",
    "print(f\"Mrr score:{mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff687816c8af4f57b165d0a73e3f47e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/kornwtp_ConGen-paraphrase-multilingual-mpnet-base-v2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'mrp/simcse-model-roberta-base-thai' # mrp/simcse-model-distil-m-bert\n",
    "model_name = 'kornwtp/ConGen-paraphrase-multilingual-mpnet-base-v2' # kornwtp/ConGen-simcse-model-roberta-base-thai\n",
    "model = SentenceTransformer(model_name)\n",
    "model.max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kornwtp/ConGen-paraphrase-multilingual-mpnet-base-v2\n",
      "Traninng Score P@1: 0.754\n",
      "Traninng Score P@5: 0.931\n",
      "Traninng Score P@10: 0.966\n",
      "Mrr score:0.830\n"
     ]
    }
   ],
   "source": [
    "doc_context_id = df_document['doc_id'].to_list()    \n",
    "doc_context_encoded = model.encode(df_document['document'].to_list(),convert_to_numpy=True,normalize_embeddings=True)\n",
    "\n",
    "question_id = df_question['doc_id'].to_list()\n",
    "questions = model.encode(df_question['question'].to_list(),convert_to_numpy=True,normalize_embeddings=True)\n",
    "\n",
    "top_1,top_5,top_10,mrr = evaluate(question_id,questions,doc_context_id,doc_context_encoded)\n",
    "\n",
    "print(f'{model_name}')\n",
    "precision = top_1 / len(questions)\n",
    "print(f\"Traninng Score P@1: {precision:.3f}\")\n",
    "precision = top_5 / len(questions)\n",
    "print(f\"Traninng Score P@5: {precision:.3f}\")\n",
    "precision = top_10 / len(questions)\n",
    "print(f\"Traninng Score P@10: {precision:.3f}\")\n",
    "print(f\"Mrr score:{mrr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client('YOUR COHERE API KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traninng Score P@1: 0.8252\n",
      "Traninng Score P@5: 0.9445\n",
      "Traninng Score P@10: 0.9613\n",
      "Mrr score:0.8778\n"
     ]
    }
   ],
   "source": [
    "bs = 96\n",
    "query = []\n",
    "doc = []\n",
    "for i in range(len(df_document['document'])//bs+1):\n",
    "    doc.append(co.embed(\n",
    "      texts=df_document['document'][(i*bs):((i+1)*bs)].values.tolist(),\n",
    "      model='embed-multilingual-v2.0',\n",
    "    ).embeddings)\n",
    "for i in range(len(df_question['question'])//bs+1):\n",
    "    query.append(co.embed(\n",
    "      texts=df_question['question'][(i*bs):((i+1)*bs)].values.tolist(),\n",
    "      model='embed-multilingual-v2.0',\n",
    "    ).embeddings)\n",
    "\n",
    "questions = np.concatenate(query,0)\n",
    "doc_context_encoded = np.concatenate(doc,0)\n",
    "\n",
    "doc_context_id = df_document['doc_id'].to_list()    \n",
    "question_id = df_question['doc_id'].to_list()\n",
    "\n",
    "top_1,top_5,top_10,mrr = evaluate(question_id,questions,doc_context_id,doc_context_encoded)\n",
    "precision = top_1 / len(questions)\n",
    "print(f\"Traninng Score P@1: {precision:.4f}\")\n",
    "precision = top_5 / len(questions)\n",
    "print(f\"Traninng Score P@5: {precision:.4f}\")\n",
    "precision = top_10 / len(questions)\n",
    "print(f\"Traninng Score P@10: {precision:.4f}\")\n",
    "print(f\"Mrr score:{mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGE M-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 23 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 105777.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traninng Score P@1: 0.9050\n",
      "Traninng Score P@5: 0.9924\n",
      "Traninng Score P@10: 0.9941\n",
      "Mrr score:0.9433\n"
     ]
    }
   ],
   "source": [
    "doc_context_id = df_document['doc_id'].to_list()    \n",
    "doc_context_encoded = model.encode(df_document['document'].to_list())['dense_vecs']\n",
    "\n",
    "question_id = df_question['doc_id'].to_list()\n",
    "questions = model.encode(df_question['question'].to_list())['dense_vecs']\n",
    "\n",
    "top_1,top_5,top_10,mrr = evaluate(question_id,questions,doc_context_id,doc_context_encoded)\n",
    "\n",
    "precision = top_1 / len(questions)\n",
    "print(f\"Traninng Score P@1: {precision:.4f}\")\n",
    "precision = top_5 / len(questions)\n",
    "print(f\"Traninng Score P@5: {precision:.4f}\")\n",
    "precision = top_10 / len(questions)\n",
    "print(f\"Traninng Score P@10: {precision:.4f}\")\n",
    "print(f\"Mrr score:{mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
