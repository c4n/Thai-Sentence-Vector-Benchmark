{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics.pairwise import (\n",
    "    paired_cosine_distances,\n",
    "    paired_euclidean_distances,\n",
    "    paired_manhattan_distances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_acc_and_threshold(scores, labels, high_score_more_similar: bool):\n",
    "    assert len(scores) == len(labels)\n",
    "    rows = list(zip(scores, labels))\n",
    "\n",
    "    rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n",
    "\n",
    "    max_acc = 0\n",
    "    best_threshold = -1\n",
    "\n",
    "    positive_so_far = 0\n",
    "    remaining_negatives = sum(np.array(labels) == 0)\n",
    "\n",
    "    for i in range(len(rows) - 1):\n",
    "        score, label = rows[i]\n",
    "        if label == 1:\n",
    "            positive_so_far += 1\n",
    "        else:\n",
    "            remaining_negatives -= 1\n",
    "\n",
    "        acc = (positive_so_far + remaining_negatives) / len(labels)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            best_threshold = (rows[i][0] + rows[i + 1][0]) / 2\n",
    "\n",
    "    return max_acc, best_threshold\n",
    "    \n",
    "def find_best_f1_and_threshold(scores, labels, high_score_more_similar: bool):\n",
    "    assert len(scores) == len(labels)\n",
    "\n",
    "    scores = np.asarray(scores)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    rows = list(zip(scores, labels))\n",
    "\n",
    "    rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n",
    "\n",
    "    best_f1 = best_precision = best_recall = 0\n",
    "    threshold = 0\n",
    "    nextract = 0\n",
    "    ncorrect = 0\n",
    "    total_num_duplicates = sum(labels)\n",
    "\n",
    "    for i in range(len(rows) - 1):\n",
    "        score, label = rows[i]\n",
    "        nextract += 1\n",
    "\n",
    "        if label == 1:\n",
    "            ncorrect += 1\n",
    "\n",
    "        if ncorrect > 0:\n",
    "            precision = ncorrect / nextract\n",
    "            recall = ncorrect / total_num_duplicates\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = precision\n",
    "                best_recall = recall\n",
    "                threshold = (rows[i][0] + rows[i + 1][0]) / 2\n",
    "\n",
    "    return best_f1, best_precision, best_recall, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_score(scores, labels, high_score_more_similar: bool):\n",
    "    return average_precision_score(labels, scores * (1 if high_score_more_similar else -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_metrics(scores, labels, high_score_more_similar):\n",
    "    \n",
    "    acc, acc_threshold = find_best_acc_and_threshold(\n",
    "        scores, labels, high_score_more_similar\n",
    "    )\n",
    "    f1, precision, recall, f1_threshold = find_best_f1_and_threshold(\n",
    "        scores, labels, high_score_more_similar\n",
    "    )\n",
    "    ap = ap_score(scores, labels, high_score_more_similar)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"accuracy_threshold\": acc_threshold,\n",
    "        \"f1\": f1,\n",
    "        \"f1_threshold\": f1_threshold,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"ap\": ap,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(dataset,model):\n",
    "    sentence1, sentence2, labels = zip(*dataset)\n",
    "    sentences1 = list(sentence1)\n",
    "    sentences2 = list(sentence2)\n",
    "    labels = [int(x) for x in labels]\n",
    "    \n",
    "    sentences = list(set(sentences1 + sentences2))\n",
    "        \n",
    "    embeddings = np.asarray(model.encode(sentences))\n",
    "    emb_dict = {sent: emb for sent, emb in zip(sentences, embeddings)}\n",
    "    embeddings1 = [emb_dict[sent] for sent in sentences1]\n",
    "    embeddings2 = [emb_dict[sent] for sent in sentences2]\n",
    "    \n",
    "    cosine_scores = 1 - paired_cosine_distances(embeddings1, embeddings2)\n",
    "    manhattan_distances = paired_manhattan_distances(embeddings1, embeddings2)\n",
    "    euclidean_distances = paired_euclidean_distances(embeddings1, embeddings2)\n",
    "    \n",
    "    embeddings1_np = np.asarray(embeddings1)\n",
    "    embeddings2_np = np.asarray(embeddings2)\n",
    "    dot_scores = [np.dot(embeddings1_np[i], embeddings2_np[i]) for i in range(len(embeddings1_np))]\n",
    "    \n",
    "    labels = np.asarray(labels)\n",
    "    output_scores = {}\n",
    "    for short_name, name, scores, reverse in [\n",
    "        [\"cos_sim\", \"Cosine-Similarity\", cosine_scores, True],\n",
    "        [\"manhattan\", \"Manhattan-Distance\", manhattan_distances, False],\n",
    "        [\"euclidean\", \"Euclidean-Distance\", euclidean_distances, False],\n",
    "        [\"dot\", \"Dot-Product\", dot_scores, True],\n",
    "    ]:\n",
    "        output_scores[short_name] = _compute_metrics(scores, labels, reverse)\n",
    "\n",
    "    return output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(dataset, model):\n",
    "    scores = compute_metrics(dataset,model)\n",
    "\n",
    "    # Main score is the max of Average Precision (AP)\n",
    "    main_score = max(scores[short_name][\"ap\"] for short_name in scores)\n",
    "    scores[\"main_score\"] = main_score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/kornwtp_ConGen-simcse-model-roberta-base-thai. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'kornwtp/ConGen-simcse-model-roberta-base-thai' # kornwtp/ConGen-simcse-model-roberta-base-thai\n",
    "model = SentenceTransformer(model_name)\n",
    "model.max_seq_length = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run data_set = 'test' and 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP score:0.6666\n"
     ]
    }
   ],
   "source": [
    "path = 'XNLI-1.0/xnli.{}.tsv' # download from https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
    "data_set = 'test'\n",
    "\n",
    "df = pd.read_csv(path.format(data_set),sep='\\t')\n",
    "df.loc[(df['language']=='th') & (df['gold_label']=='contradiction'),'label'] = '0'\n",
    "df.loc[(df['language']=='th') & (df['gold_label']=='entailment'),'label'] = '1'\n",
    "\n",
    "dataset = df[(df['language']=='th') & (df['gold_label']!='neutral')][['sentence1','sentence2','label']].values.tolist()\n",
    "\n",
    "score = cal_score(dataset, model)\n",
    "print(f\"AP score:{score['cos_sim']['ap']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP score:0.6526\n"
     ]
    }
   ],
   "source": [
    "path = 'XNLI-1.0/xnli.{}.tsv' # download from https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
    "data_set = 'dev'\n",
    "\n",
    "df = pd.read_csv(path.format(data_set),sep='\\t')\n",
    "df.loc[(df['language']=='th') & (df['gold_label']=='contradiction'),'label'] = '0'\n",
    "df.loc[(df['language']=='th') & (df['gold_label']=='entailment'),'label'] = '1'\n",
    "\n",
    "dataset = df[(df['language']=='th') & (df['gold_label']!='neutral')][['sentence1','sentence2','label']].values.tolist()\n",
    "\n",
    "score = cal_score(dataset, model)\n",
    "print(f\"AP score:{score['cos_sim']['ap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
